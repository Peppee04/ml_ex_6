{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Ensemble methods**","metadata":{}},{"cell_type":"markdown","source":"# Iris Dataset\n\nIl dataset Iris è un classico dataset nell'apprendimento automatico e nella statistica, introdotto da Ronald Fisher nel 1936. È comunemente utilizzato per attività di classificazione e clustering.\n\n## Caratteristiche e Struttura\n- **Campioni**: 150 campioni di fiori iris.\n- **Features**:\n  - Lunghezza del sepalo (cm)\n  - Larghezza del sepalo (cm)\n  - Lunghezza del petalo (cm)\n  - Larghezza del petalo (cm)\n- **Classi (Etichette Target)**:\n  - *Iris-setosa*\n  - *Iris-versicolor*\n  - *Iris-virginica*\n\nOgni classe è rappresentata da 50 campioni.\n\n## Caratteristiche Principali\n- **Balanced Dataset**: Ogni classe contiene lo stesso numero di campioni.\n- **Perfect for Beginners**: a sua semplicità e struttura ben definita lo rendono perfetto per scopi didattici.\n- **Separable Classes**:\n  - *Iris-setosa* è linearmente separabile dalle altre due classi.\n  - *Iris-versicolor* e *Iris-virginica* sono più difficili da separare tra loro.\n\n\n# Iris Dataset Classes\n\n<table>\n    <tr>\n        <th>Iris Setosa</th>\n        <th>Iris Versicolor</th>\n        <th>Iris Virginica</th>\n    </tr>\n    <tr>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFn-u9Lagrv8pV4zJ8Z1cEqXNL_uo39CrL6A&s\" alt=\"Iris setosa\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSJqxUtJiLfMX5aIoyPTPz7rMdjxgWagMlBzt0QbfATKzRqH4XnMMDN5aBrU1FvRt19jkHMOrIefjywQlDg9rOeKC6JbA72Wf--jqHD-g\" alt=\"Iris versicolor\" alt=\"Iris versicolor\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQbTwTLA7_7SeTE3B1QOKw0TlB8Rp6NU7vyg&s\" alt=\"Iris virginica\" width=\"300\" height=\"300\"></td>\n    </tr>\n</table>\n\n","metadata":{}},{"cell_type":"markdown","source":"# `plot_decision_boundary` Function\n\nLa funzione `plot_decision_boundary` permette di visualizzare i margini decisionali di un classificatore. In questo modo è possibile visualizzare come il modello distingue le feature di campioni assegnati a classi diverse.\n\n1. **Parametri**:\n    - `clf`: Il classificatore allenato che ha il metodo `predict`.\n    - `X`: La matrice dei dati di input, per cui si assume una dimensione 2D per la visualizzazione.\n    - `y`: Le labels corrispondenti ai dati `X`.\n\n2. **Output**:\n    - Un grafico 2D contenente:\n        - **Features**: `Feature 1` e `Feature 2` lungo l' asse x e y (se è stata usata PCA, queste sono le 2 componenti).\n        - **Regioni decisionali**: Colori diversi indicano regioni classificate come labels diverse.\n        - **Camponi**: I punti del dataset (`X`) sono sovrapposti alle regioni e colorati in base alla loro label corretta (`y`).\n\n### **Sintassi**\n\n```python\nplot_decision_boundary(trained_model, X_data, y_data)\n```\n","metadata":{}},{"cell_type":"code","source":"# Helper function to create the plot\n# and visualize the decision boundary\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_decision_boundary(clf, X, y):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.title(\"Decision Boundary Visualization\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:07:13.057924Z","iopub.execute_input":"2025-05-15T13:07:13.058246Z","iopub.status.idle":"2025-05-15T13:07:13.068780Z","shell.execute_reply.started":"2025-05-15T13:07:13.058222Z","shell.execute_reply":"2025-05-15T13:07:13.067198Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Funzione `train_test_split()` :\n\n---\n\n## **train_test_split()**\n\nLa funzione `train_test_split()` è parte del modulo `sklearn.model_selection`. Viene utilizzata per dividere un dataset in training e test set.\n\n\n### **Esempio**:\n```python\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training data: {len(X_train)} samples\")\nprint(f\"Testing data: {len(X_test)} samples\")\n```\n","metadata":{}},{"cell_type":"markdown","source":"# **Esercizio 1: Classifichiamo il dataset Iris con un DecisionTree**\n\nEseguire tutti gli step di preparazione per l' allenamento di un DecisionTree. Per lo split dai dati in training e test utilizzare:\n\n- `test_size` = `0.3`\n\n- `random_state` = `42`","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# svolgimento...\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nprint(f\"Training data: {len(X_train)} samples\")\nprint(f\"Testing data: {len(X_test)} samples\")\n\nscaler=StandardScaler()\nX_train_std=scaler.fit_transform(X_train)\nX_test_std=scaler.transform(X_test)\n\nn_components=2\npca = PCA(n_components)   \nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca= pca.fit_transform(X_test_std)\n\n\ndtc = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=10, random_state=43)\ndtc.fit(X_train_pca, y_train)\ntest_prediction= dtc.predict(X_test_pca)\ntest_accuracy=accuracy_score(y_test, test_prediction)\n\nprint(test_accuracy)\n\nplot_decision_boundary(dtc, X_train_pca, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:11:40.924120Z","iopub.execute_input":"2025-05-15T13:11:40.924629Z","iopub.status.idle":"2025-05-15T13:11:41.400891Z","shell.execute_reply.started":"2025-05-15T13:11:40.924594Z","shell.execute_reply":"2025-05-15T13:11:41.400064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Eesercizio 2: Implementare ensamble methods**\n\nUna volta allenato il DecisionTree nell' esercizio 1, vogliamo applicarvi i metodi di ensamble. Nello specifico andremo a implementare:\n\n* **AdaBoost**\n* **Bagging**\n* **Random Forest**\n\nDi seguito vediamo la sintassi di ognuno di questi.","metadata":{}},{"cell_type":"markdown","source":"## 1. **AdaBoostClassifier**: \n\nL' `AdaBoostClassifier` crea un insieme di alberi decisionali deboli. Assegna un peso a ciascun albero e li combina per formare un modello più robusto.\n\n### Example:\n```python\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Train the model\nada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\nada_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = ada_clf.predict(X_test)\n\n```\n\n---\n\n## 2. **RandomForestClassifier**: \n\nIl `RandomForestClassifier` costruisce più alberi in parallelo e combina i loro risultati per migliorare l'accuratezza.\n\n### Example:\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train the model\nrf_clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = rf_clf.predict(X_test)\n\n```\n\n#### **N.B. RandomForest non richiede come argomento il classificatore, differentemente dagli altri metodi.**\n\n---\n\n## 3. **BaggingClassifier**:\n\nIl `BaggingClassifier` combina molteplici modelli base (come DecisionTree) utilizzando la tecnica del bootstrapping per ridurre la varianza.\n\n### Example:\n```python\nfrom sklearn.ensemble import BaggingClassifier\n\n# Train the model\nbag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\nbag_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = bag_clf.predict(X_test)\n\n```\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n\n# Creare gli oggetti ensemble\n\n# svolgimento...\nada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\n\nrf_clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\n\nbag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n\n\n# Allenare i modelli\n\n# svolgimento...\nada_clf.fit(X_train_pca, y_train)\nrf_clf.fit(X_train_pca, y_train)\nbag_clf.fit(X_train_pca, y_train)\n\n\n# Valutare e stampare le prestazioni dei modelli\n\n# svolgimento...\n\npredictions_ada = ada_clf.predict(X_test_pca)\npredictions_rf = rf_clf.predict(X_test_pca)\npredictions_bag = bag_clf.predict(X_test_pca)\naccuracy_ada=accuracy_score(y_test,predictions_ada)\naccuracy_rf=accuracy_score(y_test, predictions_rf)\naccuracy_bag=accuracy_score(y_test, predictions_bag)\n\nprint(accuracy_ada)\nprint(accuracy_rf)\nprint(accuracy_bag)\n\nplot_decision_boundary(ada_clf, X_train_pca, y_train)\nplot_decision_boundary(rf_clf, X_train_pca, y_train)\nplot_decision_boundary(bag_clf, X_train_pca, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:21:43.189773Z","iopub.execute_input":"2025-05-15T13:21:43.191089Z","iopub.status.idle":"2025-05-15T13:21:46.032508Z","shell.execute_reply.started":"2025-05-15T13:21:43.191052Z","shell.execute_reply":"2025-05-15T13:21:46.031485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Funzioni utili per Ensemble Models\n\n---\n\n## **1. Hard Voting tra tre classificatori**\n\n### **Descrizione**:\nCombina le predizioni da tre classificatori selezionando la casse più votata per ogni campione.\n\n### **Parametri**:\n- `pred1` (numpy array): Predizioni del classificatore 1.\n- `pred2` (numpy array): Predizioni del classificatore 2.\n- `pred3` (numpy array): Predizioni del classificatore 3.\n\n### **Output**:\n- Restituisce un numpy array conenente la classe più votata per ogni campione.\n\n### **Sintassi**:\n```python\nvoted = hard_voting(pred1_test, pred2_test, pred3_test)\nprint(\"Hard Voting Predictions:\", voted)\n```\n\n---\n\n## **2. Allineare predizioni a più classi**\n\n### **Descrizione**:\nAllinea le probabilità in modo da rendere compatibili diversi subsets quando vengono combinati.\n\n### **Parametri**:\n- `pred` (numpy array): Probabilità predette da un classificatore.\n- `classes_present` (list): Classi conosciute dal classificatore.\n- `n_classes` (int, optional): Numero totale di classe, default è 3.\n\n### **Output**:\n- Ritorna un numpy array con le proabilità allineate tra tutte le classi\n- Returns a numpy array with probabilities aligned across all classes.\n\n### **Sintassi**:\n```python\naligned_probs = align_predictions(pred, [0, 1], n_classes=3)\nprint(\"Aligned Probabilities:\", aligned_probs)\n\nMatrice originale [[0.7, 0.3], [0.4, 0.6]]\ndiventa\nMatrice allineata [[0.7, 0, 0.3], [0.4, 0, 0.6]]\n\n```\n\n---\n\n## **3. Plot Decision Boundary**\n\n### **Descrizione**:\nVisualizza i margini decisionali per vari ensamble methods, inclusi `expert1`, `expert2`, `expert3`, `base`, `soft_voting`, `hard_voting`, e `gating`.\n\n### **Parametri**:\n- `X` (numpy array): Dati per la visualizzazione (2D).\n- `y` (numpy array): Lables originali.\n- `clf1`, `clf2`, `clf3` (classifiers, optional): Esperti usati per calcolare prediction.\n- `base` (classifier, optional): Classificatore base.\n- `mode` (string): Metodo di ensamble per la visualizzazione. I valori possibili sono: `\"expert1\"`, `\"expert2\"`, `\"expert3\"`, `\"base\"`, `\"soft_voting\"`, `\"hard_voting\"`, `\"gating\"`.\n\n\n### **Output**:\n- Displays a decision boundary plot for the selected mode.\n\n### **Usage**:\n```python\nplot_decision_boundary(x_test, y_test, expert1=expert1, clf2=expert2, clf3=expert3, base=base_network, mode=\"soft_voting\")\n```\n","metadata":{}},{"cell_type":"markdown","source":"# **Esercizio 3: Implementare Mixture of Experts**\n\nNel seguente esercizio vogliamo implementare un meccanismo di Mixture of Experts. Poichè in iris sono presenti 3 classi, vogliamo allenare 3 classificatori (cioè 3 esperi), rispettivamente:\n\n- esperto 1: riconosce tra la classe 0 e la classe 1.\n- esperto 2: riconosce tra la classe 0 e la classe 2.\n- esperto 3: riconosce tra la classe 1 e la classe 2.\n\nInfine utilizzare la funzione `plot_modes` per plottare le diverse modalità.","metadata":{}},{"cell_type":"code","source":"def hard_voting(pred1, pred2, pred3):\n    combined = np.vstack([pred1, pred2, pred3]).T\n    voted = []\n    for sample in combined:\n        counts = np.bincount(sample)\n        most_common_label = counts.argmax()\n        voted.append(most_common_label)\n    return np.array(voted)\n\ndef align_predictions(pred, classes_present, n_classes=3):\n    aligned = np.zeros((len(pred), n_classes))\n    for idx, class_label in enumerate(classes_present):\n        aligned[:, class_label] = pred[:, idx]\n    return aligned\n\ndef plot_modes(X, y, clf1=None, clf2=None, clf3=None, base=None, mode=\"expert1\"):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                         np.linspace(y_min, y_max, 500))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n\n    if mode.startswith(\"expert\"):\n        expert_map = {\n            \"expert1\": (clf1, [0, 1]),\n            \"expert2\": (clf2, [0, 2]),\n            \"expert3\": (clf3, [1, 2]),\n        }\n        clf, classes = expert_map[mode]\n        Z = clf.predict(grid).reshape(xx.shape)\n        title = f\"Decision Boundary - {mode.capitalize()}\"\n\n    elif mode == \"base\":\n        Z = base.predict(grid).reshape(xx.shape)\n        title = \"Decision Boundary - Base Network\"\n\n    elif mode == \"soft_voting\":\n        Z1 = align_predictions(clf1.predict_proba(grid), [0, 1])\n        Z2 = align_predictions(clf2.predict_proba(grid), [0, 2])\n        Z3 = align_predictions(clf3.predict_proba(grid), [1, 2])\n        Z = (Z1 + Z2 + Z3).argmax(axis=1).reshape(xx.shape)\n        title = \"Decision Boundary - Soft Voting\"\n\n    elif mode == \"hard_voting\":\n        Z1 = clf1.predict(grid)\n        Z2 = clf2.predict(grid)\n        Z3 = clf3.predict(grid)\n        Z = np.array([Z1, Z2, Z3])\n        Z = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=Z)\n        Z = Z.reshape(xx.shape)\n        title = \"Decision Boundary - Hard Voting\"\n\n    elif mode == \"gating\":\n        gating_weights = base.predict_proba(grid)\n        gating_weights /= gating_weights.sum(axis=1, keepdims=True)\n        Z1 = align_predictions(clf1.predict_proba(grid), [0, 1])\n        Z2 = align_predictions(clf2.predict_proba(grid), [0, 2])\n        Z3 = align_predictions(clf3.predict_proba(grid), [1, 2])\n        Z = gating_weights * Z1 + gating_weights * Z2 + gating_weights * Z3\n        Z = Z.argmax(axis=1).reshape(xx.shape)\n        title = \"Decision Boundary - Mixture of Experts (Gating)\"\n\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:25:55.477795Z","iopub.execute_input":"2025-05-15T13:25:55.478159Z","iopub.status.idle":"2025-05-15T13:25:55.494662Z","shell.execute_reply.started":"2025-05-15T13:25:55.478131Z","shell.execute_reply":"2025-05-15T13:25:55.493446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Caricare il dataset Iris\n\n# svolgimento...\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split del dataset in training e test set\n\n# svolgimento...\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nprint(f\"Training data: {len(X_train)} samples\")\nprint(f\"Testing data: {len(X_test)} samples\")\n# Applicare lo scaling e PCA\n\n# svolgimento...\nscaler=StandardScaler()\nX_train_std=scaler.fit_transform(X_train)\nX_test_std=scaler.transform(X_test)\n\nn_components=2\npca = PCA(n_components)   \nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca= pca.fit_transform(X_test_std)\n\n# Creare 3 modelli di Decision Tree, ognuno dovrà essere esperto in una coppia di classi\n# N.B. ogni esperto dovrà essere allenato su un sottoinsieme contentente solo \n# le classi di competenza\n\n# svolgimento...\n\ndtc1 = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=10, random_state=43)\ndtc1.fit(X_train_pca[(y_train==0) | (y_train==1)], y_train[(y_train==0) | (y_train==1)])\n\n\ndtc2 = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=10, random_state=43)\ndtc2.fit(X_train_pca[(y_train==0) | (y_train==2)], y_train[(y_train==0) | (y_train==2)])\n\ndtc3 = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=10, random_state=43)\ndtc3.fit(X_train_pca[(y_train==1) | (y_train==2)], y_train[(y_train==1) | (y_train==2)])\n\n\n\n# Creare un modello di Decision Tree che funge da base network\n# N.B. il base network dovrà essere allenato su tutte le classi, sarà la nostra gating network\n\n# svolgimento...\ndtc = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=10, random_state=43)\ndtc.fit(X_train_pca, y_train)\n\n\n# Estrarre le predizione di ogni esperto sul test set.\n# N.B. Vogliamo le probabilità di appartenenza a ciascuna classe, quindi useremo `predict_proba`. Inoltre \n# ogni esperto riporterà solo le classi di competenza, quindi dovremo allineare le predizioni.\n\n# svolgimento...\n\npred_1= dtc1.predict(X_test_pca)\npred_2= dtc2.predict(X_test_pca)\npred_3= dtc3.predict(X_test_pca)\n\n\ntest_prediction_1= dtc2.predict_proba(X_test_pca)\ntest_prediction_2= dtc2.predict_proba(X_test_pca)\ntest_prediction_3=dtc2.predict_proba(X_test_pca)\n\nprediction_1=align_predictions(test_prediction_1, [0,1], n_classes=3)\nprediction_2=align_predictions(test_prediction_2, [0,2], n_classes=3)\nprediction_3=align_predictions(test_prediction_3, [1,2], n_classes=3)\n\n# Estrarre le predizioni del base network sul test set\n\n# svolgimento...\ntest_prediction= dtc.predict(X_test_pca)\n\n# Usiamo hard voting per combinare le predizioni degli esperti. \n# N.B. hard voting significa che ogni esperto vota per la sua classe di competenza e il voto più comune\n# vince.\n\n# svolgimento...\n\nh_voting=hard_voting(prediction_1.argmax(axis=1), prediction_2.argmax(axis=1), prediction_3.argmax(axis=1))\nprint(h_voting)\n\n# Usiamo soft voting per combinare le predizioni degli esperti.\n# N.B. soft voting significa che il voto di ogni esperto ha lo stesso peso.\n\n# svolgimento...\n\ns_voting=(prediction_1+prediction_2+prediction_3) /3\nprint(s_voting)\n\n\n# Usiamo gating network per combinare le predizioni degli esperti.\n# N.B. la gating network calcola le probabilità di appartenenza a ciascuna classe e le usa per pesare\n# le predizioni degli esperti.\n\n# svolgimento...\nbase_prediction= dtc.predict_proba(X_test_pca)\n\npred= (base_prediction * prediction_1 +\n       base_prediction * prediction_2 +\n       base_prediction * prediction_3)\nprint(pred)\n\n\n\n#accuracy dei classificatori\naccuracy_dtc1=accuracy_score(y_test,pred_1)\naccuracy_dtc2=accuracy_score(y_test,pred_2)\naccuracy_dtc3=accuracy_score(y_test,pred_3)\naccuracy_dtc=accuracy_score(y_test,test_prediction)\n\nprint(accuracy_dtc1)\nprint(accuracy_dtc2)\nprint(accuracy_dtc3)\nprint(accuracy_dtc)\n\n\n\n\n\n\nmodes = [\"expert1\", \"expert2\", \"expert3\", \"base\", \"soft_voting\", \"hard_voting\", \"gating\"]\n\n# Plottare le decision boundaries per ogni modalità.\n\n# svolgimento...\nfor mode in modes:\n    plot_modes(X_train_pca, y_train, clf1=dtc1, clf2=dtc2, clf3=dtc3, base=dtc, mode=mode)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:58:25.943084Z","iopub.execute_input":"2025-05-15T14:58:25.943406Z","iopub.status.idle":"2025-05-15T14:58:28.750928Z","shell.execute_reply.started":"2025-05-15T14:58:25.943377Z","shell.execute_reply":"2025-05-15T14:58:28.749768Z"}},"outputs":[],"execution_count":null}]}